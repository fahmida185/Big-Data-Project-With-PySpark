# Big Data Project With PySpark
 

# Big-Data-PySpark
 

Project Description
There's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is “lightning fast cluster computing" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. You’ll use PySpark, a Python package for spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc., to interact with works of William Shakespeare, analyze Fifa football 2018 data and perform clustering of genomic datasets. At the end of this course, you will gain an in-depth understanding of PySpark and it’s application to general Big Data analysis.

1. This part introduces the exciting world of Big Data, as well as the various concepts and different frameworks for processing Big Data.I have shown why Apache Spark is considered the best framework for BigData.
2. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is the fundamental and backbone data type of this engine. This cpart introduces RDDs and shows how RDDs can be created and executed using RDD Transformations and Actions.
3. This part demonstrates Spark SQL which is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. This part shows how Spark SQL allowed me to use DataFrames in Python.
4. PySpark MLlib is the Apache Spark scalable machine learning library in Python consisting of common learning algorithms and utilities. Throughout this last part, I showed important Machine Learning algorithms. I have built a movie recommendation engine and a spam filter using k-means clustering.